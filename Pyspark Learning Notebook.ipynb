{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d885aff6-bbd5-498b-ac9f-32a48a350828",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e5d3ac-0707-49c6-9840-bf65236d3160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd27b93-9eb9-429c-b574-ddc8afcb7755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Pysparkdataframes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2658a35e-bdb1-4e7a-95c6-cc15f5bf0002",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(r\"/FileStore/tables/pysparklearning/Train.csv\", header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0ac9cb-113c-4acb-a9ed-0430861c4cd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n|Day|GrocerySales|\n+---+------------+\n|1  |8418.59     |\n|2  |8447.47     |\n|3  |8336.49     |\n|4  |8579.17     |\n|5  |8524.31     |\n|6  |8623.5      |\n|7  |8320.11     |\n|8  |8313.53     |\n|9  |8461.34     |\n|10 |8497.24     |\n|11 |8674.65     |\n|12 |8354.63     |\n|13 |8526.67     |\n|14 |8767.31     |\n|15 |8689.53     |\n|16 |8623.62     |\n|17 |8527.89     |\n|18 |8450.51     |\n|19 |8629.84     |\n|20 |8840.12     |\n+---+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddee7799-3bf2-4a63-a041-2d164af188ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n|Day|GrocerySales|\n+---+------------+\n|  1|     8418.59|\n|  2|     8447.47|\n+---+------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "type(data.select('Day','GrocerySales'))\n",
    "# data.select(['Day','GrocerySales']).show(2)\n",
    "# so if we using .select() then it is returning as a dataframe else if we simply using the data['col'] , it is taking that as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c59b04f-839b-46c8-b41d-1cd55cd2932c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Day: string (nullable = true)\n |-- GrocerySales: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5417334f-ad94-4c1c-8546-8958928bf801",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building your own schema type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6bdf33-acb9-4079-8258-5e14f0c95853",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType\n",
    "custom_schema = [StructField('Day', IntegerType(),True),\n",
    "                 StructField('GrocerySales', FloatType(), True)]\n",
    "final_schema = StructType(fields = custom_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35383c13-0aa2-4b39-bcd6-e6f681c3a851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n|summary|               Day|     GrocerySales|\n+-------+------------------+-----------------+\n|  count|               692|              692|\n|   mean|             346.5|8564.733470592795|\n| stddev|199.90747859947612|428.8156683024209|\n|    min|                 1|          6766.37|\n|    max|                99|          9290.02|\n+-------+------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac1488f5-b17f-4e10-9abc-417072b005ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n|Day|GrocerySales|\n+---+------------+\n|  4|     8579.17|\n|  5|     8524.31|\n|  6|      8623.5|\n| 11|     8674.65|\n| 13|     8526.67|\n| 14|     8767.31|\n| 15|     8689.53|\n| 16|     8623.62|\n| 17|     8527.89|\n| 19|     8629.84|\n| 20|     8840.12|\n| 21|     8768.17|\n| 22|     8891.15|\n| 23|      8851.8|\n| 24|     8734.27|\n| 25|     8719.43|\n| 32|     8542.81|\n| 33|     8544.43|\n| 54|     8850.22|\n| 55|      8818.8|\n+---+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.filter(data['GrocerySales'] > 8500).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba219c1-c415-4fd2-860f-7cfa5e9eac78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [Row(Day='688', GrocerySales='8848.98'),\n Row(Day='689', GrocerySales='8800.45'),\n Row(Day='690', GrocerySales='9197.99'),\n Row(Day='691', GrocerySales='9062.44'),\n Row(Day='692', GrocerySales='8963.12')]"
     ]
    }
   ],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c787039f-613a-457a-a278-fd7d0f0b37fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n|Day|     GrocerySales|\n+---+-----------------+\n| 26|          8481.94|\n| 27|          8383.21|\n| 28|          8343.63|\n| 29|          8333.17|\n| 30|          8497.34|\n| 31|          8326.74|\n| 32|          8542.81|\n| 33|          8544.43|\n| 34|           8330.0|\n| 35|          8005.84|\n| 36|          8269.73|\n| 37|          8385.14|\n| 38|          8259.88|\n| 39|          8411.89|\n| 40|           7967.5|\n| 41|7861.130000000001|\n| 42|7857.489999999999|\n| 43|7861.880000000001|\n| 44|          7791.16|\n| 45|          7870.36|\n+---+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView('grocerysales')\n",
    "\n",
    "results = spark.sql(\"\"\"select * from grocerysales \n",
    "                    where Day > 25\"\"\")\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a594b5a-546c-4c49-a84d-f23e20db3c23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Manipulation\n",
    "## Filtering a data frame using the filter like in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26dc5c7-7be2-497f-b116-7b0e3a340964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: ['Day', 'GrocerySales']"
     ]
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc54d448-b2c2-4c04-bb12-4f46f32449e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n|Day|GrocerySales|\n+---+------------+\n|  6|      8623.5|\n| 11|     8674.65|\n| 14|     8767.31|\n| 15|     8689.53|\n| 16|     8623.62|\n| 19|     8629.84|\n| 20|     8840.12|\n| 21|     8768.17|\n| 22|     8891.15|\n| 23|      8851.8|\n| 24|     8734.27|\n| 25|     8719.43|\n| 54|     8850.22|\n| 55|      8818.8|\n| 57|     8759.27|\n| 58|     8716.09|\n| 68|     8977.75|\n| 69|     8873.18|\n| 70|     8668.14|\n| 71|     8631.88|\n+---+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.filter( (data['GrocerySales'] > 8300)  & ~(data['GrocerySales'] < 8600)).show()\n",
    "\n",
    "\n",
    "# data.filter(data['GrocerySales'] > np.mean(data.select(['GrocerySales']))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09ae6fb-25aa-4e98-b603-5571d26ed66f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we will be using collect more than the show because collect() will give the output in a list form and then we can use asDict() to convert it into a dataframe and play with it accoridingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0b6bebf-2b3f-4e3f-ba2f-821ee382bf8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c125103f-cb65-4cf7-a0ab-b9539178d5c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- the dataframe can be easily created by a list of data points and the column names which is basically the schema\n",
    "- It can also be created from RDD's(immutable) or from other datasources.\n",
    "- toDF(schema) converts the RDD or anything to a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f670780-1dd6-4c8b-a732-365f0987013c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [('sai','reddy',23),\n",
    "        ('Tri','Reddy',24)]\n",
    "\n",
    "columns = ['FirstName', 'LastName', 'Age']\n",
    "\n",
    "# df = spark.createDataFrame(data= data, schema = columns)\n",
    "\n",
    "# emptyRDD = spark.sparkContext.emptyRDD()\n",
    "# rdd2 = spark.sparkContext.parallelize([])\n",
    "\n",
    "# convert empytRDD to dataframe\n",
    "\n",
    "# df1 = emptyRDD.toDF(schema)\n",
    "# df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dc2418-5e41-4234-8a4c-741a8ebb8176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a RDD\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f414e4a-215e-4634-9a1b-e35602338570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4470094567349576>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'RDD' object has no attribute 'show'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-4470094567349576>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m()\n\n\u001B[0;31mAttributeError\u001B[0m: 'RDD' object has no attribute 'show'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'RDD' object has no attribute 'show'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd\n",
    "# We can't see what is in RDD using show action command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da6f5fe-6b3c-40db-a3d3-5ef527d04b3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we are converting the rdd to a dataframe\n",
    "df1 = rdd.toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754abd43-1574-4185-b378-9facf6855706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType, DoubleType, CharType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('FirstName', StringType(), True),\n",
    "    StructField('LastName', StringType(), True),\n",
    "    StructField('Age', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7f68ee-84a2-47bb-9f6e-8b8577f0b05d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('Age', df1['Age'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9feebeef-e014-47c7-bbdb-5206fc816ecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "373a37fd-e70d-4292-a8c0-402cc8bdb0c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ToPandas() Dataframe\n",
    "- we can convert the pyspark dataframe to a pandas dataframe as well, becuase it will be easy for the people who know pandas. `toPandas()`\n",
    "- the difference is pyspark is faster as it uses distributed processing whereas pandas perform it on a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea2be04-2e41-4687-ae1f-5eee62f0608e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|      sai|   reddy| 23|\n|      Tri|   Reddy| 24|\n+---------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1.show()\n",
    "pandas_df1 = df1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d3685d-29a9-454a-9554-c3692c0b4e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[47]: (pandas.core.frame.DataFrame, pyspark.sql.dataframe.DataFrame)"
     ]
    }
   ],
   "source": [
    "type(pandas_df1),type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85b0b8d-cfab-4c50-8df1-9aaef26c1230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sai</td>\n",
       "      <td>reddy</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tri</td>\n",
       "      <td>Reddy</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FirstName</th>\n      <th>LastName</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sai</td>\n      <td>reddy</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tri</td>\n      <td>Reddy</td>\n      <td>24</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ad1fd7-c998-4fbb-a174-aadc9935f86b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# StructType , StructField\n",
    "\n",
    "- Structtype is a collection of structfield objects, where structfield holds the metadata of the objects which has columnname, columndatatype, nullable(true or false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606ce6c8-f393-4c7d-ada3-18e7b1aef85c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: struct (nullable = true)\n |    |-- FirstName: string (nullable = true)\n |    |-- MiddleName: string (nullable = true)\n |    |-- LastName: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+--------------------+-----+------+------+\n|                Name|  dob|gender|salary|\n+--------------------+-----+------+------+\n|    {James, , Smith}|36636|     M|  3000|\n|   {Michael, Rose, }|40288|     M|  4000|\n|{Robert, , Williams}|42114|     M|  4000|\n|{Maria, Anne, Jones}|39192|     F|  4000|\n|  {Jen, Mary, Brown}|     |     F|    -1|\n+--------------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# converting a structured pyspark dataframe to pandas dataframe\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
    "]\n",
    "\n",
    "nested_schema = StructType([\n",
    "    StructField('Name', StructType([\n",
    "        StructField('FirstName' , StringType(), True),\n",
    "        StructField('MiddleName' , StringType(), True),\n",
    "         StructField('LastName' , StringType(), True)\n",
    "    ])),\n",
    "    StructField('dob' , StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', StringType(), True)\n",
    "])\n",
    "\n",
    "struct_data = spark.createDataFrame( dataStruct, nested_schema)\n",
    "struct_data.printSchema()\n",
    "struct_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ae2f85-da64-4652-8662-d083e8770139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------\n FirstName | sai   \n LastName  | reddy \n Age       | 23    \n-RECORD 1----------\n FirstName | Tri   \n LastName  | Reddy \n Age       | 24    \n\n"
     ]
    }
   ],
   "source": [
    "df1.show(n=3,truncate=25,vertical=True) # shows the values in the spark dataframe vertically\n",
    "# The truncate keyword generally trucates the characters in the column( by default it shows 20 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f1d4e9-7b8f-4ff2-bdfd-d318e5aec8bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if you want to change or add struct to a dataframe\n",
    "df1 = df1.withColumn('Age', df1['Age'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a5539d-9099-4a62-9f45-aa6a4a1bcb48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a415ff-940b-4417-94f2-99ceee7b7e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: struct (nullable = true)\n |    |-- FirstName: string (nullable = true)\n |    |-- MiddleName: string (nullable = true)\n |    |-- LastName: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- Otherinfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: string (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+-----+--------------------+\n|                Name|  dob|           Otherinfo|\n+--------------------+-----+--------------------+\n|    {James, , Smith}|36636|{36636, M, 3000, ...|\n|   {Michael, Rose, }|40288|{40288, M, 4000, ...|\n|{Robert, , Williams}|42114|{42114, M, 4000, ...|\n|{Maria, Anne, Jones}|39192|{39192, F, 4000, ...|\n|  {Jen, Mary, Brown}|     |      {, F, -1, Low}|\n+--------------------+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# similarly we can also update existing structtype using struct\n",
    "from pyspark.sql.functions import col, struct, when\n",
    "\n",
    "updated_struct_data = struct_data.withColumn('Otherinfo',\n",
    "               struct(col(\"dob\").alias(\"identifier\"),\n",
    "                      col('gender').alias('gender'),\n",
    "                      col('salary').alias('salary'),\n",
    "                      when(col('salary').cast(IntegerType()) < 2000, 'Low')\n",
    "                      .when(col('salary').cast(IntegerType()) < 4000, 'Medium')\n",
    "                      .otherwise('High').alias('Salary_Grade')\n",
    "               )).drop('id','gender','salary')\n",
    "\n",
    "updated_struct_data.printSchema()\n",
    "updated_struct_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83054401-20e7-4546-9b3a-6e36e96ea0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can also pass ArrayType(StringType()), MapType(StringType(), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a08604e4-cf9f-49ca-b09c-3b9506a05573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating structtype from Json.\n",
    "- If we have somany columns then it is definetly hard to defind the structype and if the dataformat changes every now and then, then it will be really hard.\n",
    "- if we can load the sql structtype schema from the json file then it will be easy.\n",
    "`df.schema.json()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74f81f4a-e59e-46e3-8729-0e8c9121c7dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads(schema.json))\n",
    "df3 = spark.createDataFrame(\n",
    "        spark.sparkContext.parallelize(structureData),schemaFromJson)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae87d845-eed7-4bd2-b1ee-0c258c2a44b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to check if the column is present in dataframe\n",
    "df.schema.fieldNames.contains('colname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "773e4658-55d8-479b-a7f4-8d2e0e579a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Hghly used functionalities of Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6230f4e-4691-4d2b-af62-9fe256037e33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## df.Select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66740ef7-46ff-4ebf-a55b-2a332af25e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|      sai|   reddy| 23|\n|      Tri|   Reddy| 24|\n+---------+--------+---+\n\n+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|      sai|   reddy| 23|\n|      Tri|   Reddy| 24|\n+---------+--------+---+\n\n+---------+--------+\n|FirstName|LastName|\n+---------+--------+\n|      sai|   reddy|\n|      Tri|   Reddy|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# to see all the columns\n",
    "df1.select([col for col in df1.columns]).show()\n",
    "df1.select('*').show() \n",
    "\n",
    "# we can also select based on regex\n",
    "from pyspark.sql.functions import col\n",
    "df1.select(df1.colRegex(\"`^.*name*`\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbddf4c-c41a-420e-a3f9-1b15cd3881d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|      sai|   reddy| 23|\n|      Tri|   Reddy| 24|\n+---------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1.columns[:3]).show()\n",
    "# we can also select the columns from the struct column like name.firstname, name.lastname or name.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64441cf3-591c-432b-9d8a-13ea8fc31798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## df.collect()\n",
    "\n",
    "- It is an action operation. once it is called it triggers the compute to perform the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7b27f8-a197-4fdf-a884-2563b1843db5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "# It is used to retrieve all elements of the dataset to the drivernode\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5870979-02c1-4be7-918a-7cab101e80b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[dept_name: string, dept_id: bigint]\n"
     ]
    }
   ],
   "source": [
    "dataselect = deptDF.select('*')\n",
    "print(dataselect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ba6861-18d9-4dc4-ab6b-2d53a89f38a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- now if we notice the collect() action returned the data in a array to the driver node, whereas select is returning a Dataframe and we can't loop it.\n",
    "\n",
    "- The main difference is select() is a transformation and it returns a dataframe, but collect() is an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b44658d-4a63-4b90-be7f-b292d553c42a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(dept_name='Finance', dept_id=10)\nRow(dept_name='Marketing', dept_id=20)\nRow(dept_name='Sales', dept_id=30)\nRow(dept_name='IT', dept_id=40)\n"
     ]
    }
   ],
   "source": [
    "for i in dataCollect:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41ac39f-4440-4053-95e7-f5f60b9ef8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## withColumn\n",
    "-heavily used to change the datatypes of columns, \n",
    "  - change the value of an existing column.\n",
    "  - create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7272ef15-7d7a-412f-9376-4e56f3e0c88b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deptDF = deptDF.withColumn('dept_id_2', col('dept_id')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a8d322-cf2f-4b23-9642-dff630a15c2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[84]: DataFrame[dept_name: string, dept_id: bigint, dept_id_2: bigint]"
     ]
    }
   ],
   "source": [
    "deptDF.withColumn('dept_id_2', col('dept_id')*100) # creates new dataframe rather than updating the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2bd760-a837-4473-96de-3e3c5a57eac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+\n|dept_name|dept_id|dept_id_2|\n+---------+-------+---------+\n|  Finance|  10000|     1000|\n|Marketing|  20000|     2000|\n|    Sales|  30000|     3000|\n|       IT|  40000|     4000|\n+---------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "deptDF.withColumn('dept_id', col('dept_id')*1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c0a1dfe-ea6b-4e6c-9249-9bc3b863151f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    ".withColumnRenamed('oldcol','nw_col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b4e1ce-36d9-4f46-8669-dc60cfbfe194",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filter and where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e1274a0-2ca5-4cb0-b597-684246d2353b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- same we use df.filter(isin(), multiple coditions , ~isin(list),startswith(), endswith(),contains(),like(), rlike() - this is like with regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aa3185e-250e-42f0-a48e-c73f230c678c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col,array_contains\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "arrayStructureData = [\n",
    "        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    "        ]\n",
    "        \n",
    "arrayStructureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('languages', ArrayType(StringType()), True),\n",
    "         StructField('state', StringType(), True),\n",
    "         StructField('gender', StringType(), True)\n",
    "         ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data = arrayStructureData, schema = arrayStructureSchema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.filter(df.state == \"OH\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "df.filter(col(\"state\") == \"OH\") \\\n",
    "    .show(truncate=False)    \n",
    "    \n",
    "df.filter(\"gender  == 'M'\") \\\n",
    "    .show(truncate=False)    \n",
    "\n",
    "df.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n",
    "    .show(truncate=False)        \n",
    "\n",
    "df.filter(array_contains(df.languages,\"Java\")) \\\n",
    "    .show(truncate=False)        \n",
    "\n",
    "df.filter(df.name.lastname == \"Williams\") \\\n",
    "    .show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99627b1-0685-424f-9715-a9c737480c9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster is running\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    print('Cluster is running')\n",
    "    time.sleep(40)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ebd03e7-f745-468c-b22b-c9111d3ac57d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Distinct() , dropDuplicates() -- removes the duplicates and gives the distinct rows. dropDuplicates also supports the \n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct count: \"+str(distinctDF.count()))\n",
    "distinctDF.show(truncate=False)\n",
    "\n",
    "#Drop duplicates\n",
    "df2 = df.dropDuplicates()\n",
    "print(\"Distinct count: \"+str(df2.count()))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#Drop duplicates on selected columns\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Distinct count of department salary : \"+str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0045ec97-2c32-4987-9933-9edcd8351fb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## sort() or orderBY()\n",
    "-both performs the same operation so anyone can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24deee73-6285-48cd-8c43-f66afa144a74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|      sai|   reddy| 23|\n|      Tri|   Reddy| 24|\n+---------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53470b91-1630-41f4-a75c-cb0ed32f127b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|      Tri|   Reddy| 24|\n|      sai|   reddy| 23|\n+---------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1.sort('FirstName','Age' , ascending = [True, False]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6013f33f-ff26-4d69-b254-4c5d1824d575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|Tri      |Reddy   |24 |\n|sai      |reddy   |23 |\n+---------+--------+---+\n\n+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|sai      |reddy   |23 |\n|Tri      |Reddy   |24 |\n+---------+--------+---+\n\n+---------+--------+---+\n|FirstName|LastName|Age|\n+---------+--------+---+\n|Tri      |Reddy   |24 |\n|sai      |reddy   |23 |\n+---------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc,desc\n",
    "\n",
    "df1.sort(df1.FirstName.asc(),df1.Age.asc()).show(truncate=False)\n",
    "df1.sort(col(\"FirstName\").desc(),col(\"Age\").asc()).show(truncate=False)\n",
    "df1.orderBy(col(\"FirstName\").asc(),col(\"Age\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "089afc98-cdc4-420b-9e8c-5db48834aff9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e6b9a86-b987-45f9-b01a-3761315565b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b2aa58-0b9a-48b1-89c2-b1d371d51219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|Sales     |257000     |\n|Finance   |351000     |\n|Marketing |171000     |\n+----------+-----------+\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|Sales     |3    |\n|Finance   |4    |\n|Marketing |2    |\n+----------+-----+\n\n+----------+-----+-----------+----------+\n|department|state|sum(salary)|sum(bonus)|\n+----------+-----+-----------+----------+\n|Sales     |NY   |176000     |30000     |\n|Sales     |CA   |81000      |23000     |\n|Finance   |CA   |189000     |47000     |\n|Finance   |NY   |162000     |34000     |\n|Marketing |NY   |91000      |21000     |\n|Marketing |CA   |80000      |18000     |\n+----------+-----+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
    "\n",
    "df.groupBy(\"department\").count().show(truncate=False)\n",
    "\n",
    "\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .sum(\"salary\",\"bonus\") \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66cf86bd-8f7f-4db0-ac1c-20b4c52eb77b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\") \\\n",
    "        #  max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)\n",
    "    \n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98d9a1f0-4890-4577-b117-71fdb2166403",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Joins\n",
    "- left\n",
    "- right\n",
    "- full\n",
    "- inner\n",
    "- semi (new in pyspark)  - returns all the data in the left dataframe where there is a match in right dataframe\n",
    "- anti ( New in pyspark) - similarly returns all rows in the left dataframe where there is no match in the right dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915b43f8-0d36-49c5-a924-804317202818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prapare data \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cbec3bd-a710-4070-863a-e008148ef7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- This is very important because often we end up using multiple datasets so we need to master joins, it is similar to pandas mergings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e60fcc-59c8-4286-836b-554cfdf86048",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF , empDF.emp_dept_id == deptDF.dept_id , how = 'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc67ac5-2698-4fef-9cda-a01bfdb464a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF , empDF.emp_dept_id == deptDF.dept_id , how = 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd820b9-4ccf-4340-9f01-3d5134e1c663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF , empDF.emp_dept_id == deptDF.dept_id , how = 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe641a1-4111-44f9-9a43-7d3761bfae34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF , empDF.emp_dept_id == deptDF.dept_id , how = 'leftsemi').show() \n",
    "# gave all the records from the left which has a match in the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787bf724-3976-40c0-9437-5c85c5850d3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|     6|Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF , empDF.emp_dept_id == deptDF.dept_id , how = 'leftanti').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bad25fa6-6cdd-4bf2-b330-b626b1210080",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Are joins complete without selfjoin? Damn not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ea512b-aa6f-463b-96f3-6286dce089af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+-----+\n|emp_id|name    |emp_id|name |\n+------+--------+------+-----+\n|2     |Rose    |1     |Smith|\n|3     |Williams|1     |Smith|\n|4     |Jones   |2     |Rose |\n|5     |Brown   |2     |Rose |\n|6     |Brown   |2     |Rose |\n+------+--------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"),\\\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "    col(\"emp2.emp_id\"),\\\n",
    "    col(\"emp2.name\"))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec40bbe6-8118-4f5c-8a39-d13a27f6c5e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Union vs unionByName\n",
    "\n",
    "- union basically appends all the data based on positions.\n",
    "- unionByName appends based on column names ( they can be anywhere in the table) it also gives a keyword allowMissingColumns so that we don't get any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75729617-e174-46a6-b4ca-2c3ef2e70005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- col0: long (nullable = true)\n |-- col1: long (nullable = true)\n |-- col2: long (nullable = true)\n |-- col3: long (nullable = true)\n\n+----+----+----+----+\n|col0|col1|col2|col3|\n+----+----+----+----+\n|   5|   2|   6|null|\n|null|   6|   7|   3|\n+----+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e42e899-3cf0-4ec0-9a90-7056ddcabef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# User Defined Functions\n",
    "- most expensive operation, use it only when there is no other choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeeb1b20-0cfb-4892-a42a-032343a1f7f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John Jones   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr \n",
    "\n",
    "convertUDF = udf(lambda z : convertCase(z), StringType())\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3601431-8325-4302-b675-8a6541e6c108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Transform\n",
    "\n",
    "- basically you are forming the chain of transformations\n",
    "\n",
    "- Becasue remember pyspark has something called **lazy evaluation**, so it doesn't start executing or performing transformations until an action function is called upon it, so everytime when a transformation is run it return another RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260462b9-dcef-44da-9438-c2f333718f06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n+----------+----+--------+\n|CourseName| fee|discount|\n+----------+----+--------+\n|      Java|4000|       5|\n|    Python|4600|      10|\n|     Scala|4100|      15|\n|     Scala|4500|      15|\n|       PHP|3000|      20|\n+----------+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData , schema = columns)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d33e0a-b089-4c12-bc16-ab3dd7302436",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+--------------+\n|CourseName| fee|discount|discounted_fee|\n+----------+----+--------+--------------+\n|      Java|4000|       5|        3800.0|\n|    Python|4600|      10|        4140.0|\n|     Scala|4100|      15|        3485.0|\n|     Scala|4500|      15|        3825.0|\n|       PHP|3000|      20|        2400.0|\n+----------+----+--------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "def discounted_fee(df):\n",
    "    return df.withColumn('discounted_fee' , df.fee - df.fee*df.discount/100)\n",
    "\n",
    "df = df.transform(discounted_fee)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4b7cb7-6e6c-4e7b-9987-15cf963e78bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Other ways \n",
    "from pyspark.sql.functions import transform\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b741d87-da2b-4e8e-9ed1-a4151e033645",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Apply \n",
    "- basically the pyspark donot have a apply function functionality but however we can use the pandas api of python to use that\n",
    "-`import pyspark.pandas as ps or pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53007dcf-35a1-4a73-a9bc-5ab4b83cd377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply is basically used for performing an operation over an entire dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab569ba0-2cea-4a05-bd26-aa3c80815152",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Map\n",
    "- dataframes do not have a map function, for using map first we should convert it into a RDD and then perform the operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1b1a7c-e708-4470-9c28-b677339f573f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|    30|\n|     Anna|    Rose|     F|    41|\n|   Robert|Williams|     M|    62|\n+---------+--------+------+------+\n\n+---------------+---+------+\n|           Name|Sex|Salary|\n+---------------+---+------+\n|    James,Smith|  M|    30|\n|      Anna,Rose|  F|    41|\n|Robert,Williams|  M|    62|\n+---------------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "rdd2 = df.rdd.map(lambda x: (x[0] +\",\" + x[1] , x[2], x[3]))\n",
    "df = rdd2.toDF(['Name' , 'Sex', 'Salary'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ccd5170-3541-4ed9-82d6-ba4b1f36eb48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can also do this by passing in a function so that if we need it we can use it anywhere in the code\n",
    "\n",
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    lastName=x.lastname\n",
    "    name=firstName+\",\"+lastName\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: func1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e2bc78-bf4d-41cf-b39d-d9e0459eff83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project Gutenberg’s', 1)\n('Alice’s Adventures in Wonderland', 1)\n('Project Gutenberg’s', 2)\n('Adventures in Wonderland', 2)\n('Project Gutenberg’s', 3)\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Project Gutenberg’s\",1),\n",
    "        (\"Alice’s Adventures in Wonderland\",1),\n",
    "        (\"Project Gutenberg’s\",2),\n",
    "        (\"Adventures in Wonderland\",2),\n",
    "       (\"Project Gutenberg’s\",3)]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "883321f3-7374-4b0b-bba8-2534737fdf8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# flatMap\n",
    "- basically same as map, but it flattens out the entire RDD or dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6411f431-1f41-454b-bd06-1a6009387ed9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', 'Gutenberg’s']\n1\n['Alice’s', 'Adventures', 'in', 'Wonderland']\n1\n['Project', 'Gutenberg’s']\n2\n['Adventures', 'in', 'Wonderland']\n2\n['Project', 'Gutenberg’s']\n3\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(lambda x : (x[0].split(\" \"), x[1]))\n",
    "for row in rdd2.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233caf29-a096-49bd-ab85-01fd6e7ac6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Random Sampling\n",
    "- as a datascientist or analyst it is more often that we analyse a random sample of a population to come to some hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12cf8ede-51ae-4a94-953d-ccb4141e8601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=2), Row(id=4), Row(id=6), Row(id=15), Row(id=17), Row(id=21), Row(id=22), Row(id=23), Row(id=25), Row(id=28), Row(id=30), Row(id=59), Row(id=64), Row(id=89), Row(id=94), Row(id=98)]\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(100)\n",
    "print(df.sample(0.1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cb8639e-67bf-48e4-8cd2-c37cccf6f38d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- we can't guarentee that the sample method only returns the given fraction of records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7861a83-e2e8-4b54-90c6-642dbdf31a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sampe(withReplacement = 'if you need repeated or duplicate values', fraction = 'fraction of data needed', seed = 'to get same results')\n",
    "df.takeSample(withReplacement , num = 'number of records', seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a84ff0-19ae-46a5-bb0a-eb25a65af9d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "778c4054-a8fb-4f5c-89d4-9abb0d148685",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.fillna(value, subset)\n",
    "# we can fill different subsets with different values as well\n",
    "df.fillna(value1, subset1).fillna(value2, subset2)\n",
    "df.fillna({column : vlaue , column:value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41cc4f86-a6d3-49df-b062-06e93d1d57fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pivot and Unpivot\n",
    "\n",
    "- the pivot column distinct values will become the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f599aeb7-cb63-4293-b361-b083cf8f6e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Learning Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
