{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8841ea65-dbb9-4c2b-956d-5128ae1f342f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark Sql Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ce3cc6-457f-4862-96eb-08e4a5dfcdad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479879ff-d384-4176-8177-f5d000e97fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The aggregate functions that we use generally are\n",
    "# sum\n",
    "# sumDistinct\n",
    "# mean\n",
    "# stddev\n",
    "# variance\n",
    "# min\n",
    "# max\n",
    "# avg\n",
    "# count\n",
    "# countDistinct\n",
    "# distinct\n",
    "# first ( new in pyspark)\n",
    "# last ( new in pyspark)\n",
    "# collect_list( new in pyspark)\n",
    "# collect_set( new in pyspark)\n",
    "# skewness, kurtosis( new in pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbabacd0-c7c9-4ed7-ae99-fa69e65ea66b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generally how we use df.select(avg(df.colname).alias())\n",
    "# or we might be using with withColumn .withColumn('coname', sum() or )\n",
    "# df.groupBy('col1','col2').sum('num_col').mean('col3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f09dd5e6-8009-43d3-a76d-fe83e93e9e89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e005d52b-e332-4c6c-9734-f987db4d38a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('agg_window_datetime_functions').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a20a680-4b1f-4577-b4c5-be77c0715141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData ,  schema = schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b55936b-60f0-4a2e-85eb-2b4004414c7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 6\napprox_count_distinct: 6\napprox_count_distinct: [4600, 3000, 3900, 4100, 3300, 2000]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, countDistinct, count, collect_list, collect_set, min, max, sum, mean, variance, stddev,first, last,sumDistinct\n",
    "\n",
    "print('approx_count_distinct: ' + str(df.select(approx_count_distinct('salary')).collect()[0][0]))\n",
    "print('approx_count_distinct: ' + str(df.select(countDistinct('salary')).collect()[0][0]))\n",
    "print('approx_count_distinct: ' + str(df.select(collect_set('salary')).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6cf6b6a-eab6-4fe7-8258-4c6a21ad5cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73a6261-58a4-4cd5-878b-a1c91a6bc6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         2|\n|       Robert|     Sales|  4100|         3|\n|         Saif|     Sales|  4100|         4|\n|      Michael|     Sales|  4600|         5|\n+-------------+----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank,lag,lead\n",
    "from pyspark.sql.functions import col,desc,asc \n",
    "\n",
    "windowSpec = Window.partitionBy('department').orderBy(col('salary').desc())\n",
    "\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d65d6ca-ea0d-4c4d-8051-7afc0d534a77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+----------+----+----+\n|employee_name|department|salary|rank|dense_rank| lag|lead|\n+-------------+----------+------+----+----------+----+----+\n|        Maria|   Finance|  3000|   1|         1|   0|3300|\n|        Scott|   Finance|  3300|   2|         2|3000|3900|\n|          Jen|   Finance|  3900|   3|         3|3300|   0|\n|        Kumar| Marketing|  2000|   1|         1|   0|3000|\n|         Jeff| Marketing|  3000|   2|         2|2000|   0|\n|        James|     Sales|  3000|   1|         1|   0|3000|\n|        James|     Sales|  3000|   1|         1|3000|4100|\n|       Robert|     Sales|  4100|   3|         2|3000|4100|\n|         Saif|     Sales|  4100|   3|         2|4100|4600|\n|      Michael|     Sales|  4600|   5|         3|4100|   0|\n+-------------+----------+------+----+----------+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('rank', rank().over(windowSpec))\n",
    "df = df.withColumn('dense_rank', dense_rank().over(windowSpec))\n",
    "df = df.withColumn('lag', lag('salary',1,0).over(windowSpec))\n",
    "df = df.withColumn('lead', lead('salary',1,0).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e001ab-87fc-4e91-83bf-c9931af99fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+----------+---+----+---+------+-----+----+----+\n|employee_name|department|salary|rank|dense_rank|lag|lead|row|   avg|  sum| min| max|\n+-------------+----------+------+----+----------+---+----+---+------+-----+----+----+\n|        Maria|   Finance|  3000|   1|         1|  0|3300|  1|3400.0|10200|3000|3900|\n|        Kumar| Marketing|  2000|   1|         1|  0|3000|  1|2500.0| 5000|2000|3000|\n|        James|     Sales|  3000|   1|         1|  0|3000|  1|3760.0|18800|3000|4600|\n+-------------+----------+------+----+----------+---+----+---+------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "windowSpecAgg = Window.partitionBy('department')\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a2bed6-e88a-4ab4-9324-9500a7ce4237",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Datetime fucntions\n",
    "- to be frank we use these functions quite frequently to handle data that has timestamps and these functionalities makes our life lot easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19888168-5637-4c10-a504-372b9c32f118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# the ones that i know of are dateadd, datediff, extract,year,month, day, now, getdate(),\n",
    "\n",
    "from pyspark.sql.functions import current_date, date_add, date_sub, add_months, months_between, next_day, last_day, date_format,dayofweek,dayofmonth,dayofyear,weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5435c2f4-d9bc-44e5-a1fa-a17303861ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|      date|\n+---+----------+\n|  1|2020-02-01|\n|  2|2019-03-01|\n|  3|2021-03-01|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id','date'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba0d0b2-752b-49dd-bb35-c1aa58301e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|current_date|\n+------------+\n|  2023-12-11|\n|  2023-12-11|\n|  2023-12-11|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias('current_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159071b8-8129-4f4c-9c34-0ed30cab1827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n|date_format(date, MMM-dd-yy)|\n+----------------------------+\n|                   Feb-01-20|\n|                   Mar-01-19|\n|                   Mar-01-21|\n+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(date_format('date', 'MMM-dd-yy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c8fa6f-7dab-45e8-9609-53dd652bb0d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n|weekofyear(current_date())|\n+--------------------------+\n|                        50|\n|                        50|\n|                        50|\n+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(weekofyear(current_date())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839da06b-f1fc-4306-a423-9cd143dd9724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|      date|datediff|\n+----------+--------+\n|2020-02-01|    1409|\n|2019-03-01|    1746|\n|2021-03-01|    1015|\n+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(col(\"date\"), \n",
    "    datediff(current_date(),col(\"date\")).alias(\"datediff\")  \n",
    "  ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23daea74-4aaa-4906-8e3c-32ebeb52b9cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n|      date|add_months|sub_months|  date_add|  date_sub|\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n+----------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\"), \n",
    "    add_months(col(\"date\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"date\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"date\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"date\"),4).alias(\"date_sub\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9998cf-61bb-434f-aa07-489117befc24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\"),\n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"),\n",
    "     hour(col(\"input\")).alias(\"hour\"),\n",
    "     minute(col(\"input\")).alias(\"minute\"),\n",
    "     second(col(\"input\")).alias(\"second\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723cd377-36b8-42d8-9110-bbc83cd4973d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# converting json data\n",
    "- to get from json we use `from_json`\n",
    "- to convert to json we use`to_json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fac73b-dff0-4116-90b6-26b5335b9a99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading files from different datasources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8244a113-453c-47d4-bb6d-3e698e1d557c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- we will look on how to read a file\n",
    "- how to write to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a391676d-8a54-4fd3-8ba7-d2086bb3f067",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c00ce86c-10df-47bf-bf86-ffe172e14e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv(path='.csv', )\n",
    "#or\n",
    "spark.read.format('csv').load()\n",
    "\n",
    "# to read multiple csv files\n",
    "spark.read.csv('path1, path2, path3')\n",
    "\n",
    "# to read all files in a folder\n",
    "spark.read.csv('folderpath')\n",
    "\n",
    "\n",
    "# all the options for reading a csv file\n",
    "spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "  .csv(\".csv file\")\n",
    "# or if you have your own schema just use .schema(your_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9959a1c4-bb07-4beb-810e-db664d9e4899",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# writing to csv\n",
    "df.write.csv('filename')\n",
    "df.write.mode('overwrite').save('filepath')\n",
    "\n",
    "# we have four saving modes\n",
    "# overwrite - it overwrites\n",
    "# append - it appends\n",
    "# error - it raises an error\n",
    "#   ignore - it does nothing and it doesnot save the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4970daf-bc7e-4b07-a184-288ebf930c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d9616c-4bf2-4470-8668-62e1f9f62ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "saprk.read.parquet()\n",
    "df.write.parquet()\n",
    "\n",
    "df.write.mode('overwrite').parquet()\n",
    "\n",
    "# we can also create parquet partition files\n",
    "df.write.partitionBy().mode('overwrite').parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5806e948-8416-4103-93cc-50d390dc4366",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "befeb7f5-27fd-4215-a93e-376cee82c42c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.json()\n",
    "spark.read.option(multiline = True).load()\n",
    "spark.read.json([multiple files])\n",
    "\n",
    "\n",
    "# to wrtie to a json file\n",
    "\n",
    "df.write.json('.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "016435c5-0e7a-4329-b618-5f4acafbdf4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## to read from Hive table and write to hive\n",
    "- `enableHiveSupport()`\n",
    "- `saveAsTable()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5deed2e-a91f-4c1b-97a3-699a070d4fcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#enableHiveSupport() -> enables sparkSession to connect with Hive\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/hive/warehouse/dir\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://remote-host:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# or Use the below approach\n",
    "# Change using conf\n",
    "spark.sparkContext().conf().set(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\");\n",
    "spark.sparkContext().conf().set(\"hive.metastore.uris\", \"thrift://localhost:9083\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a7358f-cbf3-46e2-be73-da4dc2557348",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## To read and write to sql server table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba09bef5-b946-4fa4-a3c7-066b169da29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to read from table\n",
    "df = spark.read \\\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "  .option(\"url\", \"jdbc:sqlserver://{SERVER_ADDR};databaseName=emp;\") \\\n",
    "  .option(\"dbtable\", \"employee\") \\\n",
    "  .option(\"user\", \"replace_user_name\") \\\n",
    "  .option(\"password\", \"replace_password\") \\\n",
    "  .load()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# to write to table\n",
    "\n",
    "sampleDF.write \\\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", \"jdbc:sqlserver://{SERVER_ADDR};databaseName=emp;\") \\\n",
    "  .option(\"dbtable\", \"employee\") \\\n",
    "  .option(\"user\", \"replace_user_name\") \\\n",
    "  .option(\"password\", \"replace_password\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86083c05-5f2e-43bf-98aa-e8401c542cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ebb4a07-abeb-45de-a15a-61503686e408",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pyspark Built-in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfdbed77-f60c-4b36-9d11-680062018cea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## when()\n",
    "- same as case statement when(col == this, thisvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6042681e-267e-413f-aa1e-9dd7b9a05f88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"M\",60000), (\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000), (\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "#Using When otherwise\n",
    "from pyspark.sql.functions import when,col\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "                                 .when(df.gender == \"F\",\"Female\")\n",
    "                                 .when(df.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df.gender))\n",
    "df2.show()\n",
    "\n",
    "df2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n",
    "                  .when(df.gender == \"F\",\"Female\")\n",
    "                  .when(df.gender.isNull() ,\"\")\n",
    "                  .otherwise(df.gender).alias(\"new_gender\"))\n",
    "df2.show()\n",
    "# Using SQL Case When\n",
    "from pyspark.sql.functions import expr\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "          \"ELSE gender END\"))\n",
    "df3.show()\n",
    "\n",
    "df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "           \"ELSE gender END\").alias(\"new_gender\"))\n",
    "\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "              \"ELSE gender END as new_gender from EMP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ac72a22-695e-4e8c-89f8-0eceb644fee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# other powerful functions is regexp_replace(colname, oldvalue, newvalue)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregate, window functions, Datetime functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
